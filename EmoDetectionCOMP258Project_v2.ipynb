{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3cc894-aded-4189-aac3-3c318a39e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.keras import backend as K\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "643c15fa-70b6-4dff-87fd-74d16b7db917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3405a662-87f4-4fb8-a65d-f30ea51b3c1d",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b8f88a-94b0-4136-80e4-84ec3280e11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 files belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset without any predefined batch size\n",
    "train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    'images/train',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=None,  # Load as unbatched dataset\n",
    "    image_size=(48, 48),  # Resize images to standard size\n",
    "    seed=42 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34d5821-e749-46b9-ae75-dc49da74655e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 files belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = keras.utils.image_dataset_from_directory(\n",
    "    directory = 'images/test',\n",
    "    labels = 'inferred',\n",
    "    label_mode = 'int',\n",
    "    batch_size = 120,\n",
    "    image_size = (48, 48),\n",
    "    seed=42 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e6650a-aba5-4eb2-9ff8-287cf084d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names\n",
    "class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35753fa-3e8a-44b4-b093-c53378664922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 21:31:06.337529: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# Extract data from train dataset\n",
    "def extract_images_and_labels(dataset):\n",
    "    X, y = [], []\n",
    "    for image, label in dataset:\n",
    "        X.append(image.numpy())\n",
    "        y.append(label.numpy())\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Assuming train_data is loaded correctly\n",
    "X, y = extract_images_and_labels(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a74af66-57fb-4278-9469-f9dfaa4a96ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape before augmentation: (28709, 48, 48, 3)\n",
      "y shape before augmentation: (28709,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X shape before augmentation: {X.shape}')\n",
    "print(f'y shape before augmentation: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f4b0949-8c09-4614-abe3-93d906aae577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(X, y, target_count=2000, rotation_range=10, width_shift_range=0.1, height_shift_range=0.1,\n",
    "                      zoom_range=0.1, brightness_range=(0.95, 1.05), horizontal_flip=True, vertical_flip=True, fill_mode='nearest'):\n",
    "    # Initialize ImageDataGenerator\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=rotation_range,\n",
    "        width_shift_range=width_shift_range,\n",
    "        height_shift_range=height_shift_range,\n",
    "        zoom_range=zoom_range,\n",
    "        brightness_range=brightness_range,\n",
    "        horizontal_flip=horizontal_flip,\n",
    "        vertical_flip=vertical_flip,\n",
    "        fill_mode=fill_mode\n",
    "    )\n",
    "\n",
    "    balanced_X = []\n",
    "    balanced_y = []\n",
    "    \n",
    "    unique_classes = np.unique(y)\n",
    "    # For each class, augment until reaching the target count\n",
    "    for class_label in unique_classes:\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        class_images = X[class_indices]\n",
    "        class_labels = y[class_indices]\n",
    "        num_images = class_images.shape[0]\n",
    "\n",
    "        # Calculate how many augmentations are needed\n",
    "        augmentations_needed = target_count - num_images\n",
    "\n",
    "        # Add original images to balanced dataset\n",
    "        balanced_X.extend(class_images)\n",
    "        balanced_y.extend(class_labels)\n",
    "\n",
    "        # Calculate how many augmentations are needed for this class\n",
    "        augmentations_needed = target_count - num_images\n",
    "\n",
    "        # Augment images until we reach the target count\n",
    "        while augmentations_needed > 0:\n",
    "            for img, label in zip(class_images, class_labels):\n",
    "                if augmentations_needed <= 0:\n",
    "                    break\n",
    "                img = img.reshape((1,) + img.shape)  # Reshape to (1, height, width, channels)\n",
    "                label = label.reshape((1,))  # Reshape to a single label\n",
    "                augmented_img = next(datagen.flow(img))  # Generate an augmented image\n",
    "                balanced_X.append(augmented_img.squeeze())  # Add augmented image to list\n",
    "                balanced_y.append(label.squeeze())  # Add label to list\n",
    "                augmentations_needed -= 1\n",
    "\n",
    "    balanced_X = np.array(balanced_X)\n",
    "    balanced_y = np.array(balanced_y)\n",
    "\n",
    "    return balanced_X, balanced_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4734e63-b1a2-4562-9c79-035c3c43bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_augmented, y_augmented = data_augmentation(X, y, target_count=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a55035d5-e9f5-42a1-827d-b9c50e2b280a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape after augmentation: (30273, 48, 48, 3)\n",
      "y shape after augmentation: (30273,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X shape after augmentation: {X_augmented.shape}')\n",
    "print(f'y shape after augmentation: {y_augmented.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "007cd1ad-f466-4893-a1e8-246e3f987f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 7215, 4: 4965, 5: 4830, 2: 4097, 0: 3995, 6: 3171, 1: 2000})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "343b23ce-4e7c-4193-af9a-b8d257c55c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "X = np.array(X_augmented)  # Images\n",
    "y = np.array(y_augmented)  # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c371386-916e-47db-b642-5862805eecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for stratified shuffle split\n",
    "batch_size = 126\n",
    "n_splits = 1  # Number of splits you want\n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=None, train_size=batch_size / len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f95e074e-c580-4ba2-b8bc-abe298c1dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate stratified batches\n",
    "batches = []\n",
    "for _, batch_indices in sss.split(X, y):\n",
    "    batch_images = X[batch_indices].astype(np.float32)/ 255.0  # Ensure images are float32\n",
    "    batch_labels = y[batch_indices].astype(np.int32)    # Ensure labels are int32\n",
    "    batches.append((batch_images, batch_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f82cec37-9d9c-45d7-8038-67747e64cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow Dataset\n",
    "final_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        tf.convert_to_tensor([batch[0] for batch in batches], dtype=tf.float32),  # Images as float32\n",
    "        tf.convert_to_tensor([batch[1] for batch in batches], dtype=tf.int32),   # Labels as int32\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7ab4d64-28f5-42c5-93f5-3301446591ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: (30147, 48, 48, 3), Label batch shape: (30147,)\n",
      "tf.Tensor([3 6 2 ... 3 0 2], shape=(30147,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first batch\n",
    "for batch in final_dataset.take(1):\n",
    "    print(f\"Image batch shape: {batch[0].shape}, Label batch shape: {batch[1].shape}\")\n",
    "    print(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87720aae-ae63-4665-9ea4-1a771dce6886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "validation_split = 0.2\n",
    "num_batches = len(batches)\n",
    "\n",
    "val_dataset = final_dataset.take(int(num_batches * validation_split))\n",
    "train_dataset = final_dataset.skip(int(num_batches * validation_split))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d79c76e3-2ebc-4f36-93a0-4352be3f4e39",
   "metadata": {},
   "source": [
    "CNN model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb0fb068-77af-43e7-9db6-3a04f0ada159",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c516476f-4e2d-44d5-8639-7aa6c7137bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"my_model.keras\"\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b65ae293-2869-4855-8c1a-979bf22f3966",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf1c380a-590c-4ce5-b06b-62a3efc29d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b29ecebd-67f8-4b9c-8078-81e49ce212a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 46, 46, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 44, 44, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 22, 22, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 22, 22, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 20, 20, 64)        36928     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 18, 18, 64)        36928     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 6, 6, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 2, 2, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 1, 1, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 1, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              263168    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 7175      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 880,199\n",
      "Trainable params: 880,199\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def3534-6740-4644-8ca5-de678fe7bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=50, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a43042-ff59-4a84-9a08-1e248bed2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd834517-3676-4fca-9122-cdfa9610cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], color = 'red', label = 'train')\n",
    "plt.plot(history.history['val_accuracy'], color = 'blue', label = 'validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29215781-9a26-463e-8dfc-eb7fb6430dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], color = 'red', label = 'train')\n",
    "plt.plot(history.history['val_loss'], color = 'blue', label = 'validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428b3b0-9b0b-442a-835f-8408eb0025df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4267e1-8f85-408c-83ed-7a8ca1653ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
